{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11817840,"sourceType":"datasetVersion","datasetId":7422907},{"sourceType":"datasetVersion","sourceId":11870888,"datasetId":7460030}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Wandb API\")\nwandb.login(key=secret_value_0)","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-05-19T09:57:26.081348Z","iopub.execute_input":"2025-05-19T09:57:26.081603Z","iopub.status.idle":"2025-05-19T09:57:36.609690Z","shell.execute_reply.started":"2025-05-19T09:57:26.081582Z","shell.execute_reply":"2025-05-19T09:57:36.609017Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24m037\u001b[0m (\u001b[33mcs24m037-iit-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# vocab.py\n\nimport json\n\nclass CharVocab:\n    def __init__(self, tokens=None, specials=['<pad>','<sos>','<eos>','<unk>']):\n        self.specials = specials\n        self.idx2char = list(specials) + (tokens or [])\n        self.char2idx = {ch:i for i,ch in enumerate(self.idx2char)}\n\n    @classmethod\n    def build_from_texts(cls, texts):\n        chars = sorted({c for line in texts for c in line})\n        return cls(tokens=chars)\n\n    def save(self, path):\n        with open(path, 'w', encoding='utf-8') as f:\n            json.dump(self.idx2char, f, ensure_ascii=False)\n\n    @classmethod\n    def load(cls, path):\n        idx2char = json.load(open(path, encoding='utf-8'))\n        inst = cls(tokens=[])\n        inst.idx2char = idx2char\n        inst.char2idx = {c:i for i,c in enumerate(idx2char)}\n        return inst\n\n    def encode(self, text, add_sos=False, add_eos=False):\n        seq = []\n        if add_sos: seq.append(self.char2idx['<sos>'])\n        for c in text:\n            seq.append(self.char2idx.get(c, self.char2idx['<unk>']))\n        if add_eos: seq.append(self.char2idx['<eos>'])\n        return seq\n\n    def decode(self, idxs, strip_specials=True):\n        chars = [self.idx2char[i] for i in idxs]\n        if strip_specials:\n            chars = [c for c in chars if c not in self.specials]\n        return ''.join(chars)\n\n    @property\n    def pad_idx(self): return self.char2idx['<pad>']\n    @property\n    def sos_idx(self): return self.char2idx['<sos>']\n    @property\n    def eos_idx(self): return self.char2idx['<eos>']\n    @property\n    def unk_idx(self): return self.char2idx['<unk>']\n    @property\n    def size(self): return len(self.idx2char)\n","metadata":{"execution":{"iopub.status.busy":"2025-05-19T12:07:42.165595Z","iopub.execute_input":"2025-05-19T12:07:42.166441Z","iopub.status.idle":"2025-05-19T12:07:42.177545Z","shell.execute_reply.started":"2025-05-19T12:07:42.166397Z","shell.execute_reply":"2025-05-19T12:07:42.177034Z"},"trusted":true},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# data.py\n\nimport os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\nimport os\n\ndef read_tsv(path):\n    with open(path, encoding='utf-8') as f:\n        for ln in f:\n            parts = ln.strip().split('\\t')\n            if len(parts) >= 2:\n                yield parts[1], parts[0]\n\nclass Seq2SeqDataset(Dataset):\n    def __init__(self, path, src_vocab, tgt_vocab):\n        self.examples = []\n        for src, tgt in read_tsv(path):\n            src_ids = src_vocab.encode(src, add_sos=True, add_eos=True)\n            tgt_ids = tgt_vocab.encode(tgt, add_sos=True, add_eos=True)\n            self.examples.append((torch.tensor(src_ids, dtype=torch.long),\n                                   torch.tensor(tgt_ids, dtype=torch.long)))\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, idx):\n        return self.examples[idx]\n\ndef collate_fn(batch, src_vocab, tgt_vocab):\n    srcs, tgts = zip(*batch)\n    srcs_p = pad_sequence(srcs, batch_first=True, padding_value=src_vocab.pad_idx)\n    tgts_p = pad_sequence(tgts, batch_first=True, padding_value=tgt_vocab.pad_idx)\n    src_lens = torch.tensor([len(s) for s in srcs], dtype=torch.long)\n    return srcs_p, src_lens, tgts_p\n\n\n\ndef get_dataloaders(\n        lang: str = 'hi',\n        batch_size: int = 64,\n        device: str = 'cpu',          # NEW  → so we can set pin_memory\n        num_workers: int = 2,         # NEW  → use both Kaggle CPU cores\n        prefetch_factor: int = 4,     # NEW  → pre-queue 4× batch_size\n        persistent_workers: bool = True  # NEW\n    ):\n    \"\"\"\n    Build train / dev / test DataLoaders for the Dakshina transliteration task.\n    The extra kwargs keep the CPU busy and overlap data transfer with GPU work. /kaggle/input/dakshina-dataset-v1-0/dakshina_dataset_v1.0\n    \"\"\"\n    base = os.path.join(\n        '/kaggle/input/dakshina-dataset-v1-0/dakshina_dataset_v1.0',\n        lang, 'lexicons'\n    )\n\n    # --- build vocabularies on train + dev -----------------------------\n    all_src, all_tgt = [], []\n    for split in ['train', 'dev']:\n        path = os.path.join(base, f'{lang}.translit.sampled.{split}.tsv')\n        for s, t in read_tsv(path):\n            all_src.append(s)\n            all_tgt.append(t)\n\n    src_vocab = CharVocab.build_from_texts(all_src)\n    tgt_vocab = CharVocab.build_from_texts(all_tgt)\n\n    # --- common DataLoader kwargs --------------------------------------\n    loader_kwargs = dict(\n        batch_size=batch_size,\n        num_workers=num_workers,\n        prefetch_factor=prefetch_factor,\n        persistent_workers=persistent_workers,\n        pin_memory=(device == 'cuda')          # True only when you’ve enabled GPU\n    )\n\n    loaders = {}\n    for split in ['train', 'dev', 'test']:\n        path = os.path.join(base, f'{lang}.translit.sampled.{split}.tsv')\n        ds = Seq2SeqDataset(path, src_vocab, tgt_vocab)\n        loaders[split] = DataLoader(\n            ds,\n            shuffle=(split == 'train'),\n            collate_fn=lambda b: collate_fn(b, src_vocab, tgt_vocab),\n            **loader_kwargs\n        )\n\n    return loaders, src_vocab, tgt_vocab","metadata":{"execution":{"iopub.status.busy":"2025-05-19T12:09:10.328959Z","iopub.execute_input":"2025-05-19T12:09:10.329337Z","iopub.status.idle":"2025-05-19T12:09:10.342596Z","shell.execute_reply.started":"2025-05-19T12:09:10.329316Z","shell.execute_reply":"2025-05-19T12:09:10.341792Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# models.py\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass Encoder(nn.Module):\n    def __init__(self, vocab_size, emb_size, hid_size, layers=1, cell='LSTM', dropout=0.0):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n        rnn_cls = {'LSTM': nn.LSTM, 'GRU': nn.GRU}[cell]\n        self.rnn = rnn_cls(emb_size,\n                           hid_size,\n                           num_layers=layers,\n                           dropout=dropout if layers>1 else 0.0,\n                           batch_first=True,\n                           bidirectional=False)\n\n    def forward(self, src, lengths):\n        # src: [B, T], lengths: [B]\n        embedded = self.embedding(src)  # [B, T, E]\n        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n        packed_out, hidden = self.rnn(packed)\n        outputs, _ = pad_packed_sequence(packed_out, batch_first=True)  # [B, T, H]\n        return outputs, hidden\n\n\nclass BahdanauAttention(nn.Module):\n    def __init__(self, enc_hid, dec_hid):\n        super().__init__()\n        self.attn = nn.Linear(enc_hid + dec_hid, dec_hid)\n        self.v = nn.Linear(dec_hid, 1, bias=False)\n\n    def forward(self, hidden, encoder_outputs, mask):\n        # hidden: [B, H], encoder_outputs: [B, T, H], mask: [B, T]\n        B, T, H = encoder_outputs.size()\n        hidden = hidden.unsqueeze(1).repeat(1, T, 1)               # [B, T, H]\n        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # [B, T, H]\n        scores = self.v(energy).squeeze(2)                        # [B, T]\n        scores = scores.masked_fill(~mask, -1e9)\n        return torch.softmax(scores, dim=1)                       # [B, T]\n\n\nclass Decoder(nn.Module):\n    \"\"\"\n    One class, two modes:\n        • use_attn=True  – Bahdanau attention (default)\n        • use_attn=False – Plain RNN decoder (no attention)\n\n    Forward always returns (logits, hidden, attn_weights_or_None),\n    so Seq2Seq code stays unchanged.\n    \"\"\"\n    def __init__(self, vocab_size, emb_size, enc_hid, dec_hid,\n                 layers=1, cell=\"LSTM\", dropout=0.0, use_attn=True):\n        super().__init__()\n        self.use_attn = use_attn\n        self.embedding = nn.Embedding(vocab_size, emb_size)\n\n        # ----- dimensions depend on whether we concatenate context -----\n        if use_attn:\n            self.attention = BahdanauAttention(enc_hid, dec_hid)\n            rnn_input_dim = emb_size + enc_hid            # [E ⊕ Henc]\n            fc_input_dim  = dec_hid + enc_hid + emb_size  # [Hdec ⊕ Henc ⊕ E]\n        else:\n            rnn_input_dim = emb_size\n            fc_input_dim  = dec_hid + emb_size\n\n        rnn_cls = {\"LSTM\": nn.LSTM, \"GRU\": nn.GRU}[cell]\n        self.rnn = rnn_cls(rnn_input_dim, dec_hid,\n                           num_layers=layers,\n                           dropout=dropout if layers > 1 else 0.0,\n                           batch_first=True)\n        self.fc = nn.Linear(fc_input_dim, vocab_size)\n\n    def forward(self, input_token, hidden, encoder_outputs, mask):\n        \"\"\"\n        input_token : [B]\n        hidden      : tuple|tensor  initial state for this step\n        encoder_outputs : [B, Tenc, Henc]\n        mask        : [B, Tenc]  (ignored when use_attn=False)\n        \"\"\"\n        emb = self.embedding(input_token).unsqueeze(1)     # [B,1,E]\n\n        if self.use_attn:\n            # ---- additive attention ----\n            dec_h = hidden[0][-1] if isinstance(hidden, tuple) else hidden[-1]\n            attn_w = self.attention(dec_h, encoder_outputs, mask)          # [B,Tenc]\n            ctx    = torch.bmm(attn_w.unsqueeze(1), encoder_outputs)        # [B,1,Henc]\n            rnn_in = torch.cat((emb, ctx), dim=2)                           # [B,1,E+Henc]\n        else:\n            ctx = None\n            attn_w = None\n            rnn_in = emb                                                    # [B,1,E]\n\n        out, hidden = self.rnn(rnn_in, hidden)       # [B,1,Hdec]\n        out = out.squeeze(1)                         # [B,Hdec]\n        emb = emb.squeeze(1)                         # [B,E]\n\n        if self.use_attn:\n            ctx = ctx.squeeze(1)                     # [B,Henc]\n            logits = self.fc(torch.cat((out, ctx, emb), dim=1))\n        else:\n            logits = self.fc(torch.cat((out, emb), dim=1))\n\n        return logits, hidden, attn_w\n\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, pad_idx, device='cpu'):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.pad_idx = pad_idx\n        self.device = device\n\n    def forward(self, src, src_lens, tgt):\n        enc_out, hidden = self.encoder(src, src_lens)\n        mask = (src != self.pad_idx)\n        B, T = tgt.size()\n        outputs = torch.zeros(B, T-1, self.decoder.fc.out_features, device=self.device)\n        input_tok = tgt[:, 0]  # <sos>\n        for t in range(1, T):\n            out, hidden, _ = self.decoder(input_tok, hidden, enc_out, mask)\n            outputs[:, t-1] = out\n            input_tok = tgt[:, t]\n        return outputs\n\n    def infer_greedy(self, src, src_lens, tgt_vocab, max_len=50):\n        enc_out, hidden = self.encoder(src, src_lens)\n        mask = (src != self.pad_idx)\n        B = src.size(0)\n        input_tok = torch.full((B,), tgt_vocab.sos_idx, device=self.device, dtype=torch.long)\n        generated = []\n        for _ in range(max_len):\n            out, hidden, _ = self.decoder(input_tok, hidden, enc_out, mask)\n            input_tok = out.argmax(1)\n            generated.append(input_tok.unsqueeze(1))\n            if (input_tok == tgt_vocab.eos_idx).all():\n                break\n        return torch.cat(generated, dim=1)\n","metadata":{"execution":{"iopub.status.busy":"2025-05-19T12:09:15.628226Z","iopub.execute_input":"2025-05-19T12:09:15.628769Z","iopub.status.idle":"2025-05-19T12:09:15.645632Z","shell.execute_reply.started":"2025-05-19T12:09:15.628747Z","shell.execute_reply":"2025-05-19T12:09:15.645075Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import os\nos.remove(\"/kaggle/working/no_attn_best_model.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:33:21.314015Z","iopub.execute_input":"2025-05-19T12:33:21.314385Z","iopub.status.idle":"2025-05-19T12:33:21.324285Z","shell.execute_reply.started":"2025-05-19T12:33:21.314348Z","shell.execute_reply":"2025-05-19T12:33:21.323706Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# # %% ------------- piuos sweep 9 : fixed hyper-params run -------------\n# import torch, torch.nn as nn, torch.optim as optim\n# from tqdm.auto import tqdm\n\n# # Hard-coded hyper-parameters taken from the CSV (\")\n# params = dict(\n#     emb_size     = 128,\n#     hidden_size  = 512,\n#     enc_layers   = 2,\n#     cell         = \"LSTM\",\n#     dropout      = 0.5,\n#     lr           = 8e-4,\n#     batch_size   = 64,\n#     epochs       = 20              # same as in the sweep\n# )\n\n# print(\"Hyper-parameters:\", params)\n\n# # ----------------------------------------------------------------------\n# def compute_exact_accuracy(model, loader, tgt_vocab, device):\n#     model.eval(); correct = total = 0\n#     with torch.no_grad():\n#         for src, src_lens, tgt in loader:\n#             src, src_lens, tgt = (x.to(device) for x in (src, src_lens, tgt))\n#             pred = model.infer_greedy(src, src_lens, tgt_vocab, max_len=tgt.size(1))\n#             for b in range(src.size(0)):\n#                 if tgt_vocab.decode(pred[b].cpu(), True) == \\\n#                    tgt_vocab.decode(tgt[b,1:].cpu(), True):\n#                     correct += 1\n#             total += src.size(0)\n#     return correct / total if total else 0.0\n# # ----------------------------------------------------------------------\n\n# device  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# loaders, src_vocab, tgt_vocab = get_dataloaders(\n#     lang=\"hi\",\n#     batch_size=params[\"batch_size\"],\n#     device=device\n# )\n\n# enc = Encoder(src_vocab.size, params[\"emb_size\"], params[\"hidden_size\"],\n#               params[\"enc_layers\"], params[\"cell\"], params[\"dropout\"]).to(device)\n\n# dec = Decoder(tgt_vocab.size, params[\"emb_size\"],\n#               params[\"hidden_size\"], params[\"hidden_size\"],\n#               params[\"enc_layers\"], params[\"cell\"],\n#               params[\"dropout\"], use_attn=False).to(device)\n\n# model = Seq2Seq(enc, dec, pad_idx=src_vocab.pad_idx, device=device).to(device)\n\n# criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx)\n# optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n\n# # --------------------------- training loop with best model save ----------------------------\n# best_val_acc = 0.0\n# save_path = \"no_attn_best_model.pth\"\n\n# for epoch in tqdm(range(1, params[\"epochs\"] + 1), desc=\"Epochs\"):\n#     # ---- train ----\n#     model.train(); total_loss = 0\n#     for src, src_lens, tgt in tqdm(loaders[\"train\"], leave=False):\n#         src, src_lens, tgt = src.to(device), src_lens.to(device), tgt.to(device)\n#         optimizer.zero_grad()\n#         out  = model(src, src_lens, tgt)\n#         loss = criterion(out.reshape(-1, out.size(-1)), tgt[:,1:].reshape(-1))\n#         loss.backward(); nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n#         optimizer.step(); total_loss += loss.item()\n#     train_loss = total_loss / len(loaders[\"train\"])\n\n#     # ---- validation ----\n#     model.eval(); val_loss = 0\n#     with torch.no_grad():\n#         for src, src_lens, tgt in loaders[\"dev\"]:\n#             src, src_lens, tgt = src.to(device), src_lens.to(device), tgt.to(device)\n#             out = model(src, src_lens, tgt)\n#             val_loss += criterion(out.reshape(-1, out.size(-1)),\n#                                   tgt[:,1:].reshape(-1)).item()\n#     val_loss /= len(loaders[\"dev\"])\n\n#     train_acc = compute_exact_accuracy(model, loaders[\"train\"], tgt_vocab, device)\n#     val_acc   = compute_exact_accuracy(model, loaders[\"dev\"],   tgt_vocab, device)\n\n#     print(f\"Epoch {epoch:2d} | \"\n#           f\"train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | \"\n#           f\"train_acc {train_acc:.4f} | val_acc {val_acc:.4f}\")\n\n#     # ---- save best model ----\n#     if val_acc > best_val_acc:\n#         best_val_acc = val_acc\n#         torch.save(model.state_dict(), save_path)\n#         print(f\"✓ Saved new best model with val_acc = {val_acc:.4f}\")\n\n# # -------------------------------------------------------------------------------------------\n# # --------------------------- final test -------------------------------\n# test_acc = compute_exact_accuracy(model, loaders[\"test\"], tgt_vocab, device)\n# print(f\"\\n★ Test accuracy (exact match): {test_acc:.4f}\")\n# # %% -------------------------------------------------------------------\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% ------------- with out ATTN : fixed hyper-params run -------------\nimport torch, torch.nn as nn, torch.optim as optim\nfrom tqdm.auto import tqdm\n\n# Hard-coded hyper-parameters taken from the CSV (Name == \"fallen-sweep-30\")\n# params = dict(\n#     emb_size     = 128,\n#     hidden_size  = 128,\n#     enc_layers   = 1,\n#     cell         = \"LSTM\",\n#     dropout      = 0.5,\n#     lr           = 5e-4,\n#     batch_size   = 64,\n#     epochs       = 20              # same as in the sweep\n# )\n# params = dict(\n#     emb_size     = 128,\n#     hidden_size  = 512,\n#     enc_layers   = 2,\n#     cell         = \"LSTM\",\n#     dropout      = 0.5,\n#     lr           = 8e-4,\n#     batch_size   = 64,\n#     epochs       = 20              # same as in the sweep\n# )\nparams = dict(\n    emb_size     = 256,\n    hidden_size  = 512,\n    enc_layers   = 2,\n    cell         = \"LSTM\",\n    dropout      = 0.5,\n    lr           = 1e-4,\n    batch_size   = 64,\n    epochs       = 20              # same as in the sweep\n)\n\nprint(\"Hyper-parameters:\", params)\n\n# ----------------------------------------------------------------------\ndef compute_exact_accuracy(model, loader, tgt_vocab, device):\n    model.eval()\n    correct = total = 0\n    with torch.no_grad():\n        for src, src_lens, tgt in loader:\n            src, src_lens, tgt = (x.to(device) for x in (src, src_lens, tgt))\n            pred = model.infer_greedy(src, src_lens, tgt_vocab, max_len=tgt.size(1))\n\n            # iterate over the batch\n            for b in range(src.size(0)):\n                pred_str  = tgt_vocab.decode(pred[b].cpu().tolist())             # strip specials by default\n                gold_str  = tgt_vocab.decode(tgt[b, 1:].cpu().tolist())          # skip <sos>\n                correct  += (pred_str == gold_str)\n            total += src.size(0)\n\n    return correct / total if total else 0.0\n# ----------------------------------------------------------------------\nbest_val_acc = 0.0\nsave_path = \"no_attn_best_model.pth\"\n\ndevice  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nloaders, src_vocab, tgt_vocab = get_dataloaders(\n    lang=\"hi\",\n    batch_size=params[\"batch_size\"],\n    device=device\n)\n\nenc = Encoder(src_vocab.size, params[\"emb_size\"], params[\"hidden_size\"],\n              params[\"enc_layers\"], params[\"cell\"], params[\"dropout\"]).to(device)\n\ndec = Decoder(tgt_vocab.size, params[\"emb_size\"],\n              params[\"hidden_size\"], params[\"hidden_size\"],\n              params[\"enc_layers\"], params[\"cell\"],\n              params[\"dropout\"], use_attn=False).to(device)\n\nmodel = Seq2Seq(enc, dec, pad_idx=src_vocab.pad_idx, device=device).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx)\noptimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n\n# --------------------------- training loop ----------------------------\nfor epoch in tqdm(range(1, params[\"epochs\"] + 1), desc=\"Epochs\"):\n    # ---- train ----\n    model.train(); total_loss = 0\n    for src, src_lens, tgt in tqdm(loaders[\"train\"], leave=False):\n        src, src_lens, tgt = src.to(device), src_lens.to(device), tgt.to(device)\n        optimizer.zero_grad()\n        out  = model(src, src_lens, tgt)\n        loss = criterion(out.reshape(-1, out.size(-1)), tgt[:,1:].reshape(-1))\n        loss.backward(); nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step(); total_loss += loss.item()\n    train_loss = total_loss / len(loaders[\"train\"])\n\n    # ---- validation ----\n    model.eval(); val_loss = 0\n    with torch.no_grad():\n        for src, src_lens, tgt in loaders[\"dev\"]:\n            src, src_lens, tgt = src.to(device), src_lens.to(device), tgt.to(device)\n            out = model(src, src_lens, tgt)\n            val_loss += criterion(out.reshape(-1, out.size(-1)),\n                                  tgt[:,1:].reshape(-1)).item()\n    val_loss /= len(loaders[\"dev\"])\n\n    train_acc = compute_exact_accuracy(model, loaders[\"train\"], tgt_vocab, device)\n    val_acc   = compute_exact_accuracy(model, loaders[\"dev\"],   tgt_vocab, device)\n\n    print(f\"Epoch {epoch:2d} | \"\n          f\"train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | \"\n          f\"train_acc {train_acc:.4f} | val_acc {val_acc:.4f}\")\n\n# --------------------------- final test -------------------------------\ntorch.save(model.state_dict(), save_path)\ntest_acc = compute_exact_accuracy(model, loaders[\"test\"], tgt_vocab, device)\nprint(f\"\\n★ Test accuracy (exact match): {test_acc:.4f}\")\n# %% -------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:34:26.806475Z","iopub.execute_input":"2025-05-19T12:34:26.806783Z","iopub.status.idle":"2025-05-19T12:45:16.729305Z","shell.execute_reply.started":"2025-05-19T12:34:26.806759Z","shell.execute_reply":"2025-05-19T12:45:16.728282Z"}},"outputs":[{"name":"stdout","text":"Hyper-parameters: {'emb_size': 256, 'hidden_size': 512, 'enc_layers': 2, 'cell': 'LSTM', 'dropout': 0.5, 'lr': 0.0001, 'batch_size': 64, 'epochs': 20}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epochs:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7079220054a4ca4b7b41637aaf94391"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch  1 | train_loss 2.0560 | val_loss 1.1961 | train_acc 0.0811 | val_acc 0.0739\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch  2 | train_loss 0.9907 | val_loss 0.8378 | train_acc 0.1939 | val_acc 0.1799\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch  3 | train_loss 0.7381 | val_loss 0.6941 | train_acc 0.2800 | val_acc 0.2327\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch  4 | train_loss 0.6115 | val_loss 0.6238 | train_acc 0.3410 | val_acc 0.2802\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch  5 | train_loss 0.5286 | val_loss 0.5732 | train_acc 0.3964 | val_acc 0.2939\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch  6 | train_loss 0.4682 | val_loss 0.5398 | train_acc 0.4438 | val_acc 0.3249\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch  7 | train_loss 0.4184 | val_loss 0.5147 | train_acc 0.4846 | val_acc 0.3357\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch  8 | train_loss 0.3792 | val_loss 0.4999 | train_acc 0.5246 | val_acc 0.3481\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch  9 | train_loss 0.3448 | val_loss 0.4833 | train_acc 0.5607 | val_acc 0.3674\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10 | train_loss 0.3152 | val_loss 0.4754 | train_acc 0.5958 | val_acc 0.3681\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11 | train_loss 0.2896 | val_loss 0.4650 | train_acc 0.6281 | val_acc 0.3752\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12 | train_loss 0.2659 | val_loss 0.4627 | train_acc 0.6424 | val_acc 0.3749\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13 | train_loss 0.2446 | val_loss 0.4668 | train_acc 0.6749 | val_acc 0.3733\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 14 | train_loss 0.2269 | val_loss 0.4566 | train_acc 0.7098 | val_acc 0.3816\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 15 | train_loss 0.2097 | val_loss 0.4544 | train_acc 0.7229 | val_acc 0.3887\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 16 | train_loss 0.1947 | val_loss 0.4639 | train_acc 0.7486 | val_acc 0.3807\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 17 | train_loss 0.1811 | val_loss 0.4624 | train_acc 0.7669 | val_acc 0.3841\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 18 | train_loss 0.1696 | val_loss 0.4645 | train_acc 0.7730 | val_acc 0.3887\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 19 | train_loss 0.1570 | val_loss 0.4695 | train_acc 0.7900 | val_acc 0.3830\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/691 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 20 | train_loss 0.1472 | val_loss 0.4764 | train_acc 0.8071 | val_acc 0.3832\n\n★ Test accuracy (exact match): 0.3883\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os, csv, torch, random\nfrom pathlib import Path\nfrom tabulate import tabulate\n\n# from data   import get_dataloaders\n# from vocab  import CharVocab\n# from models import Encoder, Decoder, Seq2Seq\n\n# ---------- 1. specify your best hyper-params & checkpoint -------------\n# BEST_CONFIG = dict(\n#     emb_size     = 128,\n#     hidden_size  = 512,\n#     enc_layers   = 2,\n#     cell         = \"LSTM\",\n#     dropout      = 0.5,\n#     lr           = 8e-4,\n#     batch_size   = 64,\n#     epochs       = 20              # same as in the sweep\n# )\nBEST_CONFIG = dict(\n    emb_size     = 256,\n    hidden_size  = 512,\n    enc_layers   = 2,\n    cell         = \"LSTM\",\n    dropout      = 0.5,\n    lr           = 1e-4,\n    batch_size   = 64,\n    epochs       = 20              # same as in the sweep\n)\n\nrandom.seed(42)\nBEST_CKPT = \"/kaggle/working/no_attn_best_model.pth\"\nLANG      = \"hi\"\nBATCH     = 1\n# -----------------------------------------------------------------------\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nloaders, src_vocab, tgt_vocab = get_dataloaders(LANG, batch_size=BATCH, device=device)\n\n# ----- rebuild best model ------------------------------------------------\nenc = Encoder(src_vocab.size, BEST_CONFIG[\"emb_size\"],\n              BEST_CONFIG[\"hidden_size\"], BEST_CONFIG[\"enc_layers\"],\n              BEST_CONFIG[\"cell\"],  BEST_CONFIG[\"dropout\"]).to(device)\n\ndec = Decoder(tgt_vocab.size, BEST_CONFIG[\"emb_size\"],\n              BEST_CONFIG[\"hidden_size\"],BEST_CONFIG['hidden_size'],\n              BEST_CONFIG[\"enc_layers\"], BEST_CONFIG[\"cell\"],\n              BEST_CONFIG[\"dropout\"],use_attn=False).to(device)\n\nmodel = Seq2Seq(enc, dec, pad_idx=src_vocab.pad_idx, device=device).to(device)\nmodel.load_state_dict(torch.load(BEST_CKPT, map_location=device))\nmodel.eval()\n\n# ---------- 2. accuracy on test set --------------------------------------\ncorrect = total = 0\npred_rows = []\nall_samples = []  # collect all for random sampling later\n\nwith torch.no_grad():\n    for i, (src, src_len, tgt) in enumerate(loaders[\"test\"]):\n        src, src_len, tgt = (x.to(device) for x in (src, src_len, tgt))\n\n        pred = model.infer_greedy(src, src_len, tgt_vocab, max_len=tgt.size(1))\n        pred_str  = tgt_vocab.decode(pred[0].cpu(), strip_specials=True)\n        gold_str  = tgt_vocab.decode(tgt[0, 1:].cpu(), strip_specials=True)\n        input_str = src_vocab.decode(src[0].cpu(), strip_specials=True)\n        is_corr   = pred_str == gold_str\n        \n        correct += int(is_corr)\n        total   += 1\n\n        row = (i, input_str, gold_str, pred_str, is_corr)\n        pred_rows.append(row)\n        all_samples.append(row)\n\ntest_acc = correct / total\nprint(f\"\\nExact-match TEST accuracy: {test_acc*100:.2f}% ({correct}/{total})\\n\")\n\n# ---------- 3. pretty 30 random-row sample table --------------------------\nrandom.seed(42)  # ensures reproducibility of sample\nsample_rows = random.sample(all_samples, 30)\n\nheaders = [\"idx\", \"input (src)\", \"gold (tgt)\", \"prediction\", \"correct\"]\nprint(tabulate(sample_rows, headers=headers, tablefmt=\"github\"))\n\n# ---------- 4. save ALL test predictions ----------------------------------\nout_dir = Path(\"predictions_vanilla\"); out_dir.mkdir(exist_ok=True)\nout_file = out_dir / \"test_predictions.tsv\"\nwith out_file.open(\"w\", encoding=\"utf-8\", newline=\"\") as f:\n    writer = csv.writer(f, delimiter=\"\\t\")\n    writer.writerow(headers)           # header row\n    writer.writerows(pred_rows)\n\nprint(f\"\\nSaved {len(pred_rows)} predictions to {out_file.absolute()}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:51:33.348240Z","iopub.execute_input":"2025-05-19T12:51:33.348513Z","iopub.status.idle":"2025-05-19T12:52:01.723885Z","shell.execute_reply.started":"2025-05-19T12:51:33.348492Z","shell.execute_reply":"2025-05-19T12:52:01.723028Z"}},"outputs":[{"name":"stdout","text":"\nExact-match TEST accuracy: 39.38% (1773/4502)\n\n|   idx | input (src)   | gold (tgt)   | prediction   | correct   |\n|-------|---------------|--------------|--------------|-----------|\n|   912 | khatakati     | खटकती         | खताकती         | False     |\n|   204 | avrodhak      | अवरोधक        | अवरोधक        | True      |\n|  2253 | parosane      | परोसने         | परोसने         | True      |\n|  2006 | najibabad     | नजीबाबाद        | नज़ीबाबाद        | False     |\n|  1828 | dikhlaa       | दिखला          | दिखला          | True      |\n|  1143 | chidchidapan  | चिडचिडापन       | चिड़चिड़ापन       | False     |\n|   839 | katrina       | कैटरीना         | कतरिना         | False     |\n|  4467 | hippo         | हिप्पो          | हिप्पो          | True      |\n|   712 | kalamjeet     | कमलजीत        | कलमाजीत        | False     |\n|  3456 | reeta         | रीता           | रीता           | True      |\n|   260 | aakaash       | आकाश          | आकाश          | True      |\n|   244 | aset          | असेट          | असेत          | False     |\n|   767 | kazi          | काजी           | काजी           | True      |\n|  1791 | darp          | दर्प          | डर्प          | False     |\n|  1905 | deshkal       | देशकाल         | देशकाल         | True      |\n|  4139 | simmons       | साइमंस         | सिमंस          | False     |\n|   217 | avika         | अविका          | अविका          | True      |\n|  1628 | dual          | ड्युअल         | दुल           | False     |\n|  4464 | hijada        | हिजड़ा          | हिजड़ा          | True      |\n|  3436 | richual       | रिचुअल         | रिचुल          | False     |\n|  1805 | dakhile       | दाखिले          | दख़िले          | False     |\n|  3679 | laurence      | लौरेंस          | लारेंस          | False     |\n|  2278 | pahnakar      | पहनाकर        | पहनकर        | False     |\n|    53 | agnat         | अज्ञात         | अज्ञात         | True      |\n|  1307 | jalvaayu      | जलवायु         | जलवायु         | True      |\n|  3462 | rugna         | रुग्ण          | रूग्ण          | False     |\n|  2787 | bapaa         | बापा           | बापा           | True      |\n|  2276 | pehnate       | पहनते         | पहनेत         | False     |\n|  1273 | jamaate       | जमाती          | जमाते          | False     |\n|  1763 | theatrs       | थियेटर्स        | थिट्रस         | False     |\n\nSaved 4502 predictions to /kaggle/working/predictions_vanilla/test_predictions.tsv\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# # %% [markdown]\n# # ------------------- Error-analysis helpers -------------------\n# import collections, math\n# import pandas as pd\n# from tabulate import tabulate\n# import matplotlib.pyplot as plt\n\n# # 1) Build a DataFrame for convenience\n# df = pd.DataFrame(pred_rows, columns=[\"idx\", \"src\", \"gold\", \"pred\", \"correct\"])\n\n# # 2) Word-length buckets ------------------------------------------------\n# df[\"len\"] = df[\"gold\"].str.len()\n# bins   = [0, 5, 8, 12, 20, math.inf]             # choose cut-points that make sense for your data\n# labels = [\"≤5\", \"6–8\", \"9–12\", \"13–20\", \"21+\"]\n# df[\"len_bin\"] = pd.cut(df[\"len\"], bins=bins, labels=labels, right=True)\n# acc_by_len = df.groupby(\"len_bin\")[\"correct\"].mean().reset_index()\n\n# print(\"\\n### Accuracy vs word length\")\n# print(tabulate(acc_by_len, headers=[\"len_bin\", \"accuracy\"], tablefmt=\"github\"))\n\n# # 3) Character-level confusion matrix -----------------------------------\n# def char_confusions(row):\n#     return [(g, p) for g, p in zip(row.gold, row.pred) if g != p]\n\n# conf_counter = collections.Counter()\n# df[~df.correct].apply(lambda r: conf_counter.update(char_confusions(r)), axis=1)\n\n# # Show 15 most common confusions\n# top_15 = conf_counter.most_common(15)\n# print(\"\\n### Top 15 character confusions (gold → pred)\")\n# print(tabulate([(g, p, c) for (g, p), c in top_15],\n#                headers=[\"gold\", \"pred\", \"count\"], tablefmt=\"github\"))\n\n# # 4) Vowel vs consonant error rates -------------------------------------\n# # — define your own sets if the default doesn't match the language —\n# vowels = set(\"aeiouAEIOU\")            # replace with Hi/Deva vowels if needed\n# def is_vowel(ch): return ch in vowels\n\n# vowel_errs = conson_errs = total_v = total_c = 0\n# for _, row in df.iterrows():\n#     for g, p in zip(row.gold, row.pred):\n#         if is_vowel(g):\n#             total_v += 1\n#             vowel_errs += (g != p)\n#         else:\n#             total_c += 1\n#             conson_errs += (g != p)\n# print(\"\\n### Vowel vs consonant character error rate\")\n# print(f\"vowels      : {vowel_errs/total_v:.3%} ({vowel_errs}/{total_v})\")\n# print(f\"consonants  : {conson_errs/total_c:.3%} ({conson_errs}/{total_c})\")\n\n# # 5) Optional: heat-map style confusion plot ----------------------------\n# # This is entirely optional; comment out if you don’t need a figure.\n# import numpy as np\n# chars = sorted({g for g, _ in conf_counter} | {p for _, p in conf_counter})\n# char2idx = {ch:i for i,ch in enumerate(chars)}\n# mat = np.zeros((len(chars), len(chars)), dtype=int)\n# for (g,p), cnt in conf_counter.items():\n#     mat[char2idx[g], char2idx[p]] = cnt\n\n# plt.figure(figsize=(8,8))\n# plt.imshow(mat, interpolation='nearest')\n# plt.title(\"Character-level confusion (gold rows → pred cols)\")\n# plt.xticks(range(len(chars)), chars, rotation=90, fontsize=6)\n# plt.yticks(range(len(chars)), chars, fontsize=6)\n# plt.xlabel(\"predicted\"); plt.ylabel(\"gold\"); plt.tight_layout()\n# plt.show()\n\n\n\n\n# ───────────────────────── Error-analysis cell ─────────────────────────\n# Drop this cell **below** the test-prediction loop.\n# It expects `pred_rows` to be a list of tuples:\n#   (idx, input_str, gold_str, pred_str, correct_bool)\n# and will print quick diagnostics + an optional confusion heat-map.\n\nimport collections, math\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tabulate import tabulate\n\n# 0) Put the raw rows into a DataFrame for convenience  ────────────────\ndf = pd.DataFrame(\n    pred_rows, columns=[\"idx\", \"src\", \"gold\", \"pred\", \"correct\"]\n)\n\ndf[\"len\"] = df[\"gold\"].str.len()\nmax_len  = df[\"len\"].max()\n\nbins   = [0, 5, 8, 12, 20]\nif max_len > 20:\n    bins.append(max_len + 1)\nelse:\n    bins.append(21)\n\nlabels = [\"≤5\", \"6–8\", \"9–12\", \"13–20\", f\"21+ ({max_len})\"]\ndf[\"len_bin\"] = pd.cut(df[\"len\"], bins=bins, labels=labels, right=True)\n\nacc_by_len = (df.groupby(\"len_bin\", observed=False)[\"correct\"]\n                .mean()\n                .reset_index())\n\nprint(\"\\n### Accuracy vs word length\")\nprint(tabulate(acc_by_len, headers=[\"len_bin\", \"accuracy\"], tablefmt=\"github\"))\n\n# 2) Character-level confusion counts  ─────────────────────────────────\ndef char_confusions(row):\n    return [(g, p) for g, p in zip(row.gold, row.pred) if g != p]\n\nconf_counter = collections.Counter()\ndf[~df.correct].apply(lambda r: conf_counter.update(char_confusions(r)), axis=1)\n\ntop_k = 15\nprint(f\"\\n### Top {top_k} character confusions (gold → pred)\")\nprint(tabulate(\n    [(g, p, c) for (g, p), c in conf_counter.most_common(top_k)],\n    headers=[\"gold\", \"pred\", \"count\"], tablefmt=\"github\"\n))\n\n# 3) Vowel vs consonant error rate  ───────────────────────────────────\n# Basic Devanagari vowels + common matras\nvowel_set = set(\"अआइईउऊऋॠऌऍएऐऑओऔ\") | set(\"ािीुूेैोौंःृॅॉ\")\ndef is_vowel(ch): return ch in vowel_set\n\nvowel_errs = conson_errs = total_v = total_c = 0\nfor _, row in df.iterrows():\n    for g, p in zip(row.gold, row.pred):\n        if is_vowel(g):\n            total_v  += 1\n            vowel_errs += (g != p)\n        else:\n            total_c  += 1\n            conson_errs += (g != p)\n\nprint(\"\\n### Vowel vs consonant character error rate\")\nif total_v > 0:\n    print(f\"vowels      : {vowel_errs/total_v:.3%} ({vowel_errs}/{total_v})\")\nif total_c > 0:\n    print(f\"consonants  : {conson_errs/total_c:.3%} ({conson_errs}/{total_c})\")\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T13:30:58.213074Z","iopub.execute_input":"2025-05-19T13:30:58.213338Z","iopub.status.idle":"2025-05-19T13:30:58.488072Z","shell.execute_reply.started":"2025-05-19T13:30:58.213322Z","shell.execute_reply":"2025-05-19T13:30:58.486748Z"}},"outputs":[{"name":"stdout","text":"\n### Accuracy vs word length\n|    | len_bin   |   accuracy |\n|----|-----------|------------|\n|  0 | ≤5        |   0.45245  |\n|  1 | 6–8       |   0.358085 |\n|  2 | 9–12      |   0.266304 |\n|  3 | 13–20     |   0        |\n|  4 | 21+ (15)  | nan        |\n\n### Top 15 character confusions (gold → pred)\n| gold   | pred   |   count |\n|--------|--------|---------|\n| र      | ा       |     127 |\n| ा       | र      |     112 |\n| ट      | त      |     108 |\n| ि       | ी       |     107 |\n| न      | ा       |     105 |\n| ा       | न      |     103 |\n| ी       | ि       |      96 |\n| ा       | त      |      71 |\n| ु       | ू       |      69 |\n| ू       | ु       |      68 |\n| ड      | द      |      67 |\n| र      | ्       |      63 |\n| ल      | ा       |      62 |\n| ो       | ं       |      59 |\n| ्       | र      |      55 |\n\n### Vowel vs consonant character error rate\nvowels      : 31.637% (2860/9040)\nconsonants  : 22.820% (3730/16345)\n","output_type":"stream"}],"execution_count":29},{"cell_type":"markdown","source":"✅ Final Answer – Part (c): Error Analysis\nInsightful bullet points:\n\nAccuracy drops with word length.\nShort words (≤5 characters) achieve 45.2% accuracy, while accuracy drops to 35.8% for 6–8, 26.6% for 9–12, and 0% for words longer than 12. This shows the model struggles with maintaining alignment on longer sequences, likely due to limitations of plain RNNs without attention.\n\nFrequent confusion between र and ा.\nThe model mistakenly predicts र→ा 127 times and ा→र 112 times. These characters are phonetically and visually distinct, so the error suggests misalignment rather than linguistic confusion.\n\nRetroflex vs dental stop confusion.\nCommon phonetic errors include ट→त (108×) and ड→द (67×), reflecting the difficulty of distinguishing retroflex and dental stops from Latin transcriptions like \"t\" and \"d\".\n\nMatra (vowel mark) length mismatches are frequent.\nExamples: ि→ी (107×), ु→ू (69×), and ा→त, ा→न, etc. The model often predicts the correct base vowel but gets the length wrong—likely due to overfitting on dominant training patterns.\n\nHallucination or omission of र and ् (halant).\nThe confusion matrix shows र→् and ्→र both occur over 50 times. This could arise from noisy alignment between Latin script clusters (like \"ra\", \"ri\", etc.) and Devanagari representations.\n\nVowels are harder than consonants.\nSurprisingly, the vowel character error rate is 31.6%, while consonant error rate is only 22.8%. This contradicts typical assumptions and may reflect difficulty in modeling matras and their positions in the sequence.\n\nNo predictions are correct for long words (13+ characters).\nEven though some examples were attempted, none matched exactly. This is a strong signal that greedy decoding without attention struggles to capture long-range dependencies.\n\nLet me know if you want to turn this into a formatted LaTeX report/table or need similar analysis for your attention-based model!","metadata":{}},{"cell_type":"code","source":"# import matplotlib as mpl\n# import matplotlib.pyplot as plt\n\n\n\n# font_path = \"/kaggle/input/hindhi/NotoSansDevanagari-Regular.ttf\"\n# mpl.font_manager.fontManager.addfont(font_path)\n# mpl.rcParams[\"font.family\"] = \"Noto Sans Devanagari\"\n\n# plt.figure()\n# plt.text(0.5, 0.5, \"नमस्ते\", fontsize=40, ha=\"center\")\n# plt.axis(\"off\")\n# plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Question 5 (c) Sample Grid\n\nimport matplotlib as mpl\n\n# Path to the local font file on Kaggle /kaggle/input/hindhi/NotoSansDevanagari-Regular.ttf\nfont_path = \"/kaggle/input/hindhi/NotoSansDevanagari-Regular.ttf\"\n\n# Register the font with matplotlib\nmpl.font_manager.fontManager.addfont(font_path)\nmpl.rcParams[\"font.family\"] = \"Noto Sans Devanagari\"\n\n# plot_attention_grid.py\nimport torch, matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport random \nimport numpy as np\n# from data   import get_dataloaders\n# from models import Encoder, Decoder, Seq2Seq   # same definitions you trained with\n\n# ---------- 1. specify your best hyper-params & checkpoint -------------\nBEST_CONFIG = dict(\n    emb_size     = 256,\n    hidden_size  = 512,\n    enc_layers   = 2,\n    cell         = \"LSTM\",\n    dropout      = 0.5,\n    lr           = 1e-4,\n    batch_size   = 64,\n    epochs       = 20              # same as in the sweep\n)\n\nrandom.seed(42)\nBEST_CKPT = \"/kaggle/working/no_attn_best_model.pth\"\nLANG      = \"hi\"\nBATCH     = 1\n# -----------------------------------------------------------------------\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nloaders, src_vocab, tgt_vocab = get_dataloaders(LANG, batch_size=BATCH, device=device)\n\n# ----- rebuild best model ------------------------------------------------\nenc = Encoder(src_vocab.size, BEST_CONFIG[\"emb_size\"],\n              BEST_CONFIG[\"hidden_size\"], BEST_CONFIG[\"enc_layers\"],\n              BEST_CONFIG[\"cell\"],  BEST_CONFIG[\"dropout\"]).to(device)\n\ndec = Decoder(tgt_vocab.size, BEST_CONFIG[\"emb_size\"],\n              BEST_CONFIG[\"hidden_size\"],BEST_CONFIG['hidden_size'],\n              BEST_CONFIG[\"enc_layers\"], BEST_CONFIG[\"cell\"],\n              BEST_CONFIG[\"dropout\"], use_attn=False).to(device)\n\nmodel = Seq2Seq(enc, dec, pad_idx=src_vocab.pad_idx, device=device).to(device)\nmodel.load_state_dict(torch.load(BEST_CKPT, map_location=device))\nmodel.eval()\n\n\n# helper: returns prediction + attn matrix\ndef infer_with_attn(src, src_len, max_len=50):\n    \"\"\"Returns (generated token ids, attention matrix)\"\"\"\n\n    enc_out, enc_hidden = model.encoder(src, src_len)\n\n    # ---- local bridge: resize enc_hidden -> dec_hidden depth --------------\n    dec_layers = model.decoder.rnn.num_layers\n    def resize(t):\n        if t.size(0) == dec_layers:\n            return t\n        elif t.size(0) < dec_layers:                  # repeat last layer\n            pad = t[-1:].expand(dec_layers - t.size(0), -1, -1)\n            return torch.cat([t, pad], 0)\n        else:                                         # slice extra layers\n            return t[:dec_layers]\n\n    if isinstance(enc_hidden, tuple):                # LSTM (h,c)\n        h, c = enc_hidden\n        dec_hidden = (resize(h), resize(c))\n    else:                                            # GRU\n        dec_hidden = resize(enc_hidden)\n    # ----------------------------------------------------------------------\n\n    mask = (src != model.pad_idx)\n    toks, attn_rows = [], []\n    tok = torch.full((1,), tgt_vocab.sos_idx, device=device)\n\n    for _ in range(max_len):\n        out, dec_hidden, attn = model.decoder(tok, dec_hidden, enc_out, mask)\n        attn_rows.append(attn.squeeze(0).cpu())      # [T_src]\n        tok = out.argmax(1)\n        if tok.item() == tgt_v.eos_idx:\n            break\n        toks.append(tok.item())\n\n    return toks, torch.stack(attn_rows)              # [T_tgt, T_src]\n# --------------------------------------------------------------------------\n\n# 3) collect first 9 samples ---------------------------------------------------\nimport random\n\n# Convert the test loader to a list (this loads everything into memory)\nall_test_samples = list(loaders[\"test\"])\n\n# Randomly select 9 unique samples\nrandom_samples = random.sample(all_test_samples, 9)\n\n# Format them like your original loop\nsamples = [(i, src.to(device), src_len.to(device)) for i, (src, src_len, _) in enumerate(random_samples)]\n\n# 4) plot a 3×3 grid -----------------------------------------------------------\nfig, axes = plt.subplots(3, 3, figsize=(12, 10))\n\nfor (idx, src, src_len), ax in zip(samples, axes.flatten()):\n    gen_ids, attn = infer_with_attn(src, src_len)      # attn: [T_tgt, T_src]\n    attn_np = attn.detach().numpy()\n\n    # decode strings\n    src_txt = src_vocab.decode(src[0].cpu(), strip_specials=True)\n    tgt_txt = tgt_vocab.decode(gen_ids,           strip_specials=True)\n\n    T_src, T_tgt = len(src_txt), len(tgt_txt)\n\n    # plot heat-map\n    ax.imshow(attn_np, aspect='auto', cmap='Blues_r')\n\n    # put chars on axes\n    ax.set_xticks(np.arange(T_src))\n    ax.set_xticklabels(list(src_txt), fontsize=11)\n    ax.set_yticks(np.arange(T_tgt))\n    ax.set_yticklabels(list(tgt_txt), fontsize=11)\n\n    ax.xaxis.tick_top()\n    plt.setp(ax.get_xticklabels(), ha=\"center\", va=\"bottom\")\n\n    # faint grid\n    ax.set_xticks(np.arange(-.5, T_src, 1), minor=True)\n    ax.set_yticks(np.arange(-.5, T_tgt, 1), minor=True)\n    # ax.grid(which=\"minor\", color=\"white\", lw=0.5)\n    ax.tick_params(which=\"minor\", bottom=False, left=False)\n\n    # ax.set_title(f\"idx {idx}\", fontsize=10)\n\nplt.tight_layout()\nplt.savefig(\"attention_grid.png\", dpi=160)\nprint(\"Saved attention_grid.png\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T12:57:52.033393Z","iopub.execute_input":"2025-05-19T12:57:52.033739Z","iopub.status.idle":"2025-05-19T12:58:03.597830Z","shell.execute_reply.started":"2025-05-19T12:57:52.033704Z","shell.execute_reply":"2025-05-19T12:58:03.596523Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_35/3996956957.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m     \u001b[0mgen_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfer_with_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# attn: [T_tgt, T_src]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m     \u001b[0mattn_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_35/3996956957.py\u001b[0m in \u001b[0;36minfer_with_attn\u001b[0;34m(src, src_len, max_len)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mattn_rows\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# [T_src]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mtok\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtgt_v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meos_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'squeeze'"],"ename":"AttributeError","evalue":"'NoneType' object has no attribute 'squeeze'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x1000 with 9 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"# # %% ------------- with ATTN : fixed hyper-params run -------------\n# import torch, torch.nn as nn, torch.optim as optim\n# from tqdm.auto import tqdm\n\n# # Hard-coded hyper-parameters taken from the CSV (Name == \"fallen-sweep-30\")\n# params = dict(\n#     emb_size     = 128,\n#     hidden_size  = 128,\n#     enc_layers   = 1,\n#     cell         = \"LSTM\",\n#     dropout      = 0.5,\n#     lr           = 5e-4,\n#     batch_size   = 64,\n#     epochs       = 20              # same as in the sweep\n# )\n\n# print(\"Hyper-parameters:\", params)\n\n# # ----------------------------------------------------------------------\n# def compute_exact_accuracy(model, loader, tgt_vocab, device):\n#     model.eval(); correct = total = 0\n#     with torch.no_grad():\n#         for src, src_lens, tgt in loader:\n#             src, src_lens, tgt = (x.to(device) for x in (src, src_lens, tgt))\n#             pred = model.infer_greedy(src, src_lens, tgt_vocab, max_len=tgt.size(1))\n#             for b in range(src.size(0)):\n#                 if tgt_vocab.decode(pred[b].cpu(), True) == \\\n#                    tgt_vocab.decode(tgt[b,1:].cpu(), True):\n#                     correct += 1\n#             total += src.size(0)\n#     return correct / total if total else 0.0\n# # ----------------------------------------------------------------------\n\n# device  = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n# loaders, src_vocab, tgt_vocab = get_dataloaders(\n#     lang=\"hi\",\n#     batch_size=params[\"batch_size\"],\n#     device=device\n# )\n\n# enc = Encoder(src_vocab.size, params[\"emb_size\"], params[\"hidden_size\"],\n#               params[\"enc_layers\"], params[\"cell\"], params[\"dropout\"]).to(device)\n\n# dec = Decoder(tgt_vocab.size, params[\"emb_size\"],\n#               params[\"hidden_size\"], params[\"hidden_size\"],\n#               params[\"enc_layers\"], params[\"cell\"],\n#               params[\"dropout\"], use_attn=True).to(device)\n\n# model = Seq2Seq(enc, dec, pad_idx=src_vocab.pad_idx, device=device).to(device)\n\n# criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx)\n# optimizer = optim.Adam(model.parameters(), lr=params[\"lr\"])\n\n# # --------------------------- training loop ----------------------------\n# for epoch in tqdm(range(1, params[\"epochs\"] + 1), desc=\"Epochs\"):\n#     # ---- train ----\n#     model.train(); total_loss = 0\n#     for src, src_lens, tgt in tqdm(loaders[\"train\"], leave=False):\n#         src, src_lens, tgt = src.to(device), src_lens.to(device), tgt.to(device)\n#         optimizer.zero_grad()\n#         out  = model(src, src_lens, tgt)\n#         loss = criterion(out.reshape(-1, out.size(-1)), tgt[:,1:].reshape(-1))\n#         loss.backward(); nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n#         optimizer.step(); total_loss += loss.item()\n#     train_loss = total_loss / len(loaders[\"train\"])\n\n#     # ---- validation ----\n#     model.eval(); val_loss = 0\n#     with torch.no_grad():\n#         for src, src_lens, tgt in loaders[\"dev\"]:\n#             src, src_lens, tgt = src.to(device), src_lens.to(device), tgt.to(device)\n#             out = model(src, src_lens, tgt)\n#             val_loss += criterion(out.reshape(-1, out.size(-1)),\n#                                   tgt[:,1:].reshape(-1)).item()\n#     val_loss /= len(loaders[\"dev\"])\n\n#     train_acc = compute_exact_accuracy(model, loaders[\"train\"], tgt_vocab, device)\n#     val_acc   = compute_exact_accuracy(model, loaders[\"dev\"],   tgt_vocab, device)\n\n#     print(f\"Epoch {epoch:2d} | \"\n#           f\"train_loss {train_loss:.4f} | val_loss {val_loss:.4f} | \"\n#           f\"train_acc {train_acc:.4f} | val_acc {val_acc:.4f}\")\n\n# # --------------------------- final test -------------------------------\n# test_acc = compute_exact_accuracy(model, loaders[\"test\"], tgt_vocab, device)\n# print(f\"\\n★ Test accuracy (exact match): {test_acc:.4f}\")\n# # %% -------------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T10:08:52.257377Z","iopub.execute_input":"2025-05-19T10:08:52.258700Z","iopub.status.idle":"2025-05-19T10:08:52.265223Z","shell.execute_reply.started":"2025-05-19T10:08:52.258654Z","shell.execute_reply":"2025-05-19T10:08:52.264369Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# sweep_config.py  – with tqdm progress bars\nimport wandb, torch, torch.nn as nn, torch.optim as optim\nfrom tqdm.auto import tqdm               # ← new\n# from data import get_dataloaders\n# from models import Encoder, Decoder, Seq2Seq\n\ndef compute_exact_accuracy(model, loader, tgt_vocab, device):\n    model.eval()\n    correct = total = 0\n    with torch.no_grad():\n        for src, src_lens, tgt in loader:\n            src, src_lens, tgt = (x.to(device) for x in (src, src_lens, tgt))\n            pred = model.infer_greedy(src, src_lens, tgt_vocab, max_len=tgt.size(1))\n\n            # iterate over the batch\n            for b in range(src.size(0)):\n                pred_str  = tgt_vocab.decode(pred[b].cpu().tolist())             # strip specials by default\n                gold_str  = tgt_vocab.decode(tgt[b, 1:].cpu().tolist())          # skip <sos>\n                correct  += (pred_str == gold_str)\n            total += src.size(0)\n\n    return correct / total if total else 0.0\n\n\ndef objective():\n    run = wandb.init()\n    cfg = run.config\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    loaders, src_vocab, tgt_vocab = get_dataloaders('hi', \n                                                    batch_size=cfg.batch_size,\n                                                    device=device)\n\n    enc = Encoder(src_vocab.size, cfg.emb_size, cfg.hidden_size,\n                  cfg.enc_layers, cfg.cell, cfg.dropout).to(device) \n    dec = Decoder(tgt_vocab.size, cfg.emb_size, cfg.hidden_size, cfg.hidden_size,\n                  cfg.enc_layers, cfg.cell, cfg.dropout, use_attn=False).to(device)\n    model      = Seq2Seq(enc, dec, pad_idx=src_vocab.pad_idx, device=device).to(device)\n    criterion  = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx)\n    optimizer  = optim.Adam(model.parameters(), lr=cfg.lr)\n\n    # ---------- epoch loop with tqdm ----------\n    for epoch in tqdm(range(1, cfg.epochs + 1), desc=\"Epochs\", position=0):\n        model.train()\n        total_loss = 0.0\n\n        # training batches progress-bar\n        train_loader = tqdm(loaders['train'], desc=f\"Train {epoch}\", leave=False, position=1)\n        for src, src_lens, tgt in train_loader:\n            src, src_lens, tgt = src.to(device), src_lens.to(device), tgt.to(device)\n\n            optimizer.zero_grad()\n            output = model(src, src_lens, tgt)\n            loss   = criterion(output.reshape(-1, output.size(-1)), tgt[:,1:].reshape(-1))\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            total_loss += loss.item()\n        train_loader.close()\n\n        train_loss = total_loss / len(loaders['train'])\n\n        # ---------- validation loss ----------\n        val_loss = 0.0\n        val_loader = tqdm(loaders['dev'], desc=f\"Val   {epoch}\", leave=False, position=1)\n        model.eval()\n        with torch.no_grad():\n            for src, src_lens, tgt in val_loader:\n                src, src_lens, tgt = src.to(device), src_lens.to(device), tgt.to(device)\n                output  = model(src, src_lens, tgt)\n                val_loss += criterion(output.reshape(-1, output.size(-1)),\n                                      tgt[:,1:].reshape(-1)).item()\n        val_loader.close()\n        val_loss /= len(loaders['dev'])\n\n        # ---------- accuracies ----------\n        train_acc = compute_exact_accuracy(model, loaders['train'], tgt_vocab, device)\n        val_acc   = compute_exact_accuracy(model, loaders['dev'],   tgt_vocab, device)\n\n        wandb.log({\n            'epoch':      epoch,\n            'train_loss': train_loss,\n            'val_loss':   val_loss,\n            'train_acc':  train_acc,\n            'val_acc':    val_acc\n        })\n\n    wandb.finish()\n\nif __name__ == \"__main__\":\n    sweep_cfg = {\n        'method': 'bayes',\n        'metric': {'name': 'val_acc', 'goal': 'maximize'},\n        'parameters': {\n            'emb_size':    {'values': [64, 128, 256]},\n            'hidden_size': {'values': [128, 256, 512]},\n            'enc_layers':  {'values': [1, 2]},\n            # 'dec_layers':  {'values': [1, 2]},\n            'cell':        {'values': ['LSTM', 'GRU']},\n            'dropout':     {'values': [0.1, 0.3, 0.5]},\n            'lr':          {'values': [ 8e-4, 5e-4, 1e-4]},\n            'batch_size':  {'values': [32, 64]},\n            'epochs':      {'value': 20}\n        }\n    }\n\n    sweep_id = wandb.sweep(\n        sweep_cfg,\n        entity='cs24m037-iit-madras',\n        project='DA6401_A3'\n    )\n    wandb.agent(sweep_id, function=objective, count=15)","metadata":{"execution":{"iopub.status.busy":"2025-05-19T10:08:52.266143Z","iopub.execute_input":"2025-05-19T10:08:52.266468Z","iopub.status.idle":"2025-05-19T10:08:52.284996Z","shell.execute_reply.started":"2025-05-19T10:08:52.266443Z","shell.execute_reply":"2025-05-19T10:08:52.284440Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}